{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65b277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Done ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# 1) Configuration & Reproducibility\n",
    "import os, random, json, math, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "CFG = {\n",
    "    \"paths\": {\n",
    "        # Root directory containing videos like: case_<VideoID>.mp4\n",
    "        \"video_root\": \"data/videos\",\n",
    "        # Annotations CSV with columns: VideoID, Start, End, PhaseID (Start/End at 25 fps)\n",
    "        \"annotations\": \"data/annotations.csv\",\n",
    "        \"out_dir\": \"outputs_swin\"\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"fps_original\": 25,\n",
    "        \"fps_sample\": 10,        # extracted fps\n",
    "        \"clip_len\": 32,          # frames per clip (~3.2 s at 10 fps)\n",
    "        \"resize\": [224, 224],\n",
    "        \"num_classes\": 10,\n",
    "        \"val_split\": 0.2,\n",
    "        \"stride_eval\": 8          # temporal stride for evaluation stitching\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"batch_size\": 4,\n",
    "        \"num_workers\": 4,\n",
    "        \"seed\": 42,\n",
    "        \"epochs\": 25,\n",
    "        \"patience\": 10,\n",
    "        \"lr\": 3e-4,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"label_smoothing\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "Path(CFG[\"paths\"][\"out_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG[\"train\"][\"seed\"])\n",
    "\n",
    "print(\"Done ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9658b62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# 2) Dataset utilities: loading annotations, frame sampling, clip extraction\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class SurgicalClips(Dataset):\n",
    "    \"\"\"Dataset that samples fixed-length clips from phase segments within videos.\n",
    "       Assumes an annotations CSV with columns: VideoID, Start, End, PhaseID (1..num_classes), at 25 fps.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, video_root, clip_len=32, fps_sample=10, resize=(224,224),\n",
    "                 transform=None, split_vids=None, mode=\"train\"):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.data = self.data.sort_values([\"VideoID\", \"Start\"]).reset_index(drop=True)\n",
    "        if split_vids is not None:\n",
    "            self.data = self.data[self.data[\"VideoID\"].isin(split_vids)].reset_index(drop=True)\n",
    "        self.video_root = Path(video_root)\n",
    "        self.clip_len = clip_len\n",
    "        self.fps_sample = fps_sample\n",
    "        self.resize = tuple(resize)\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.fps_annot = 25\n",
    "\n",
    "        # Pre-build an index of (video_path, start_frame, phase_label) for faster __getitem__\n",
    "        self.index = []\n",
    "        for _, row in self.data.iterrows():\n",
    "            vid = int(row.VideoID)\n",
    "            path = self.video_root / f\"case_{vid}.mp4\"\n",
    "            seg_start = int(row.Start * self.fps_sample / self.fps_annot)\n",
    "            seg_end   = int(row.End   * self.fps_sample / self.fps_annot)\n",
    "            if seg_end <= seg_start:\n",
    "                continue\n",
    "            # Build multiple starting points per segment (overlapping windows) to expand data\n",
    "            step = self.clip_len // 2  # 50% overlap\n",
    "            starts = list(range(seg_start, max(seg_start+1, seg_end - self.clip_len + 1), step))\n",
    "            if len(starts) == 0:\n",
    "                starts = [max(seg_start, seg_end - self.clip_len)]\n",
    "            y = int(row.PhaseID) - 1  # to 0..num_classes-1\n",
    "            for s in starts:\n",
    "                self.index.append((path, s, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def _read_clip(self, path, start):\n",
    "        cap = cv2.VideoCapture(str(path))\n",
    "        if not cap.isOpened():\n",
    "            raise FileNotFoundError(f\"Video not found: {path}\")\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "        frames = []\n",
    "        for _ in range(self.clip_len):\n",
    "            ret, f = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "            f = cv2.resize(f, self.resize, interpolation=cv2.INTER_AREA)\n",
    "            frames.append(f)\n",
    "        cap.release()\n",
    "        # Pad/trim to fixed length\n",
    "        if len(frames) == 0:\n",
    "            # create a dummy black clip if read fails\n",
    "            frames = [np.zeros((self.resize[1], self.resize[0], 3), dtype=np.uint8) for _ in range(self.clip_len)]\n",
    "        if len(frames) < self.clip_len:\n",
    "            last = frames[-1]\n",
    "            while len(frames) < self.clip_len:\n",
    "                frames.append(last)\n",
    "        frames = frames[:self.clip_len]\n",
    "        x = np.stack(frames)  # [T, H, W, C]\n",
    "        x = torch.tensor(x).permute(0,3,1,2).float() / 255.0  # [T, C, H, W]\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, start, y = self.index[idx]\n",
    "        x = self._read_clip(path, start)\n",
    "        # apply per-frame transforms if provided\n",
    "        if self.transform is not None:\n",
    "            x = torch.stack([self.transform(frame) for frame in x])  # [T, C, H, W]\n",
    "        x = x.permute(1,0,2,3).contiguous()  # [C, T, H, W]\n",
    "        return x, y\n",
    "\n",
    "print(\"Done ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe14ce68-d3a0-4e7a-941f-b56b6af0779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['VideoID', 'FrameNo', 'Phase'], dtype='object')\n",
      "   VideoID  FrameNo  Phase\n",
      "0      269       68      1\n",
      "1      269     1043      2\n",
      "2      269     1228      3\n",
      "3      269     2118      4\n",
      "4      269     3478      5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ann = pd.read_csv(\"data/annotations.csv\", sep=';')\n",
    "ann.columns = ann.columns.str.strip().str.replace(\"\\ufeff\", \"\", regex=True)\n",
    "print(ann.columns)\n",
    "print(ann.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83fb24ee-a68f-411b-9a33-44bb7690cf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Segment-level annotations saved: 1173\n"
     ]
    }
   ],
   "source": [
    "segments = []\n",
    "for vid, df in ann.groupby(\"VideoID\"):\n",
    "    df = df.sort_values(\"FrameNo\").reset_index(drop=True)\n",
    "    start_idx = 0\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, \"Phase\"] != df.loc[i-1, \"Phase\"]:\n",
    "            phase = int(df.loc[i-1, \"Phase\"])\n",
    "            start = int(df.loc[start_idx, \"FrameNo\"])\n",
    "            end = int(df.loc[i-1, \"FrameNo\"])\n",
    "            segments.append((vid, start, end, phase))\n",
    "            start_idx = i\n",
    "    if len(df) > 0:\n",
    "        phase = int(df.loc[len(df)-1, \"Phase\"])\n",
    "        start = int(df.loc[start_idx, \"FrameNo\"])\n",
    "        end = int(df.loc[len(df)-1, \"FrameNo\"])\n",
    "        segments.append((vid, start, end, phase))\n",
    "\n",
    "segments_df = pd.DataFrame(segments, columns=[\"VideoID\", \"Start\", \"End\", \"PhaseID\"])\n",
    "segments_df.to_csv(\"data/annotations_segments.csv\", index=False)\n",
    "print(\"‚úÖ Segment-level annotations saved:\", len(segments_df))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7ff46d-524e-4bdd-b2e5-49f2698225a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "CFG[\"paths\"][\"annotations\"] = \"data/annotations_segments.csv\"\n",
    "print(\"Done ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a297cbb-758c-431d-8918-6a43e2c2b171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import os, torch, torch.multiprocessing as mp\n",
    "mp.set_sharing_strategy('file_system')     # avoid /dev/shm\n",
    "os.environ.setdefault(\"PYTORCH_DISABLE_SHM\", \"1\")  # extra guard (harmless if ignored)\n",
    "import cv2\n",
    "cv2.setNumThreads(0)  # make OpenCV behave in workers\n",
    "print(\"Done ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d2d66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train videos: 101, Val videos: 88\n",
      "Train label distribution:\n",
      " 1      82\n",
      "2     173\n",
      "3      83\n",
      "4      81\n",
      "5      82\n",
      "6      96\n",
      "7      88\n",
      "8      83\n",
      "9      84\n",
      "10     86\n",
      "Name: count, dtype: int64\n",
      "Val label distribution:\n",
      " 1     21\n",
      "2     43\n",
      "3     21\n",
      "4     20\n",
      "5     21\n",
      "6     24\n",
      "7     22\n",
      "8     21\n",
      "9     21\n",
      "10    21\n",
      "Name: count, dtype: int64\n",
      "Train clips: 1323 Val clips: 1122\n",
      "Per-class counts (train): [  19  146    0    0   40    0    0   52   37 1029]\n",
      "Done ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3) Train/Val split (Segment-level stratification) + Balanced sampler\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load annotations\n",
    "# ------------------------------------------------------------\n",
    "ann = pd.read_csv(CFG[\"paths\"][\"annotations\"]).sort_values([\"VideoID\", \"Start\"]).reset_index(drop=True)\n",
    "\n",
    "# Each row corresponds to a labeled segment with a PhaseID\n",
    "X = ann.index.values                      # segment indices\n",
    "y = ann[\"PhaseID\"].values                 # labels per segment\n",
    "\n",
    "# Stratified segment-level split\n",
    "train_idx, val_idx = train_test_split(\n",
    "    X,\n",
    "    test_size=CFG[\"data\"][\"val_split\"],\n",
    "    random_state=CFG[\"train\"][\"seed\"],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Extract unique VideoIDs from those segments\n",
    "train_vids = ann.loc[train_idx, \"VideoID\"].unique()\n",
    "val_vids   = ann.loc[val_idx,   \"VideoID\"].unique()\n",
    "\n",
    "print(f\"Train videos: {len(train_vids)}, Val videos: {len(val_vids)}\")\n",
    "print(\"Train label distribution:\\n\", pd.Series(y[train_idx]).value_counts().sort_index())\n",
    "print(\"Val label distribution:\\n\",   pd.Series(y[val_idx]).value_counts().sort_index())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Data augmentation\n",
    "# ------------------------------------------------------------\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(CFG[\"data\"][\"resize\"][0], scale=(0.6,1.0), antialias=True),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.Normalize(mean=[0.45,0.45,0.45], std=[0.225,0.225,0.225])\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(CFG[\"data\"][\"resize\"][0], antialias=True),\n",
    "    transforms.CenterCrop(CFG[\"data\"][\"resize\"][0]),\n",
    "    transforms.Normalize(mean=[0.45,0.45,0.45], std=[0.225,0.225,0.225])\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Dataset objects\n",
    "# ------------------------------------------------------------\n",
    "train_ds = SurgicalClips(\n",
    "    CFG[\"paths\"][\"annotations\"], CFG[\"paths\"][\"video_root\"],\n",
    "    clip_len=CFG[\"data\"][\"clip_len\"], fps_sample=CFG[\"data\"][\"fps_sample\"],\n",
    "    resize=CFG[\"data\"][\"resize\"], transform=train_tf, split_vids=train_vids, mode=\"train\"\n",
    ")\n",
    "\n",
    "val_ds = SurgicalClips(\n",
    "    CFG[\"paths\"][\"annotations\"], CFG[\"paths\"][\"video_root\"],\n",
    "    clip_len=CFG[\"data\"][\"clip_len\"], fps_sample=CFG[\"data\"][\"fps_sample\"],\n",
    "    resize=CFG[\"data\"][\"resize\"], transform=val_tf, split_vids=val_vids, mode=\"val\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Weighted sampler (balances per-class frequency in training)\n",
    "# ------------------------------------------------------------\n",
    "labels = [y for _, y in train_ds]\n",
    "class_counts = np.bincount(labels, minlength=CFG[\"data\"][\"num_classes\"])\n",
    "class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "sample_weights = [class_weights[y] for y in labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CFG[\"train\"][\"batch_size\"],\n",
    "    sampler=sampler,          # balanced sampling\n",
    "    shuffle=False,            # must be False when using sampler\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=None,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=CFG[\"train\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=None,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Weighted CrossEntropy loss (inverse-frequency)\n",
    "# ------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "phase_counts = ann[\"PhaseID\"].value_counts().sort_index()\n",
    "weights = 1.0 / phase_counts\n",
    "weights = weights / weights.sum()\n",
    "weights_tensor = torch.tensor(weights.values, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    weight=weights_tensor,\n",
    "    label_smoothing=CFG[\"train\"][\"label_smoothing\"]\n",
    ")\n",
    "\n",
    "print(\"Train clips:\", len(train_ds), \"Val clips:\", len(val_ds))\n",
    "print(\"Per-class counts (train):\", class_counts)\n",
    "print(\"Done ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7bb8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading pretrained VideoMAE model: MCG-NJU/videomae-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 68e369e5-859e-4ba9-bb31-f9e670c74e1b)')' thrown while requesting HEAD https://huggingface.co/MCG-NJU/videomae-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 07c38e2f-4d2e-4566-8a92-860bad64f2d7)')' thrown while requesting HEAD https://huggingface.co/MCG-NJU/videomae-base/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Inferred temporal=7.995 ‚Üí rounded to 8\n",
      "‚öôÔ∏è Padded 1 tokens to fit reshape grid (1568)\n",
      "‚ÑπÔ∏è Target grid: T=16, H=14, W=14\n",
      "‚úÖ Resized pos_embed: 1567 ‚Üí 3136\n",
      "‚úÖ Model ready with dropout, partial freeze, and stronger regularization\n"
     ]
    }
   ],
   "source": [
    "# 4) Build Video Swin model (pretrained) and replace head\n",
    "from transformers import VideoMAEForVideoClassification, get_cosine_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch import amp\n",
    "\n",
    "def build_videomae(num_classes=10, pretrained=True):\n",
    "    \"\"\"\n",
    "    VideoMAE base backbone for surgical video classification.\n",
    "    Includes dropout + partial freeze for better generalization.\n",
    "    \"\"\"\n",
    "    model_name = \"MCG-NJU/videomae-base\"\n",
    "    print(f\"üîπ Loading pretrained VideoMAE model: {model_name}\")\n",
    "\n",
    "    model = VideoMAEForVideoClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "    )\n",
    "\n",
    "    # # ---- Replace classifier head with Dropout + Linear ----\n",
    "    # in_features = (\n",
    "    #     model.classifier.in_features\n",
    "    #     if hasattr(model.classifier, \"in_features\")\n",
    "    #     else model.classifier[0].in_features\n",
    "    # )\n",
    "    # model.classifier = nn.Sequential(\n",
    "    #     nn.Dropout(0.3),                        # üîπ new: strong regularization\n",
    "    #     nn.Linear(in_features, num_classes)\n",
    "    # )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_videomae(num_classes=CFG[\"data\"][\"num_classes\"], pretrained=True)\n",
    "\n",
    "# ---- Resize positional embeddings ----\n",
    "def resize_videomae_pos_embed(model, new_temporal_len=32, new_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Fully safe VideoMAE positional embedding resizer.\n",
    "    Works even if pretrained grid is non-integer (e.g. 7.84√ó14√ó14).\n",
    "    \"\"\"\n",
    "    pos_embed = model.videomae.embeddings.position_embeddings  # [1, N, D]\n",
    "    embed_dim = pos_embed.shape[-1]\n",
    "    pos_tokens = pos_embed[:, 1:, :]  # drop cls token\n",
    "    num_patches_old = pos_tokens.shape[1]\n",
    "\n",
    "    patch_size = model.config.patch_size\n",
    "    tubelet_size = model.config.tubelet_size\n",
    "    img_size = model.config.image_size\n",
    "    old_spatial = img_size // patch_size\n",
    "\n",
    "    # infer temporal dimension as float, then round safely\n",
    "    old_temporal_f = num_patches_old / (old_spatial * old_spatial)\n",
    "    old_temporal = int(round(old_temporal_f))\n",
    "    print(f\"‚ÑπÔ∏è Inferred temporal={old_temporal_f:.3f} ‚Üí rounded to {old_temporal}\")\n",
    "\n",
    "    # pad or trim to make reshape valid\n",
    "    needed = old_temporal * old_spatial * old_spatial\n",
    "    if needed > num_patches_old:\n",
    "        pad = needed - num_patches_old\n",
    "        pos_tokens = torch.cat([pos_tokens, pos_tokens[:, :pad, :]*0], dim=1)\n",
    "        print(f\"‚öôÔ∏è Padded {pad} tokens to fit reshape grid ({needed})\")\n",
    "    elif needed < num_patches_old:\n",
    "        pos_tokens = pos_tokens[:, :needed, :]\n",
    "        print(f\"‚öôÔ∏è Trimmed {num_patches_old - needed} tokens to fit reshape grid ({needed})\")\n",
    "\n",
    "    # reshape + interpolate\n",
    "    pos_tokens = pos_tokens.reshape(1, old_temporal, old_spatial, old_spatial, embed_dim).permute(0, 4, 1, 2, 3)\n",
    "    new_temporal = new_temporal_len // tubelet_size\n",
    "    new_spatial = new_size[0] // patch_size\n",
    "    print(f\"‚ÑπÔ∏è Target grid: T={new_temporal}, H={new_spatial}, W={new_spatial}\")\n",
    "\n",
    "    pos_tokens = F.interpolate(\n",
    "        pos_tokens,\n",
    "        size=(new_temporal, new_spatial, new_spatial),\n",
    "        mode=\"trilinear\",\n",
    "        align_corners=False\n",
    "    )\n",
    "\n",
    "    pos_tokens = pos_tokens.permute(0, 2, 3, 4, 1).reshape(1, -1, embed_dim)\n",
    "    model.videomae.embeddings.position_embeddings = torch.nn.Parameter(pos_tokens)\n",
    "    print(f\"‚úÖ Resized pos_embed: {num_patches_old} ‚Üí {pos_tokens.shape[1]}\")\n",
    "\n",
    "    \n",
    "resize_videomae_pos_embed(model, new_temporal_len=CFG[\"data\"][\"clip_len\"], new_size=(224, 224))\n",
    "\n",
    "model = model.to(device)\n",
    "# ---- Partial freeze (keep early layers fixed) ----\n",
    "for name, param in model.videomae.named_parameters():\n",
    "    if \"encoder.layer\" in name and int(name.split(\".\")[2]) < 8:\n",
    "        param.requires_grad = False     # freeze first 8 blocks\n",
    "    else:\n",
    "        param.requires_grad = True      # train last 4 blocks + norm/head\n",
    "\n",
    "# ---- Optimizer with different LRs ----\n",
    "optimizer = AdamW([\n",
    "    {'params': model.videomae.parameters(), 'lr': 1e-5},  # was 1e-5\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4},\n",
    "], weight_decay=1e-3) \n",
    "\n",
    "# ---- LR scheduler ----\n",
    "num_warmup_steps = len(train_loader) * 3\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=len(train_loader) * CFG[\"train\"][\"epochs\"]\n",
    ")\n",
    "\n",
    "# ---- AMP GradScaler ----\n",
    "scaler = amp.GradScaler(device=\"cuda\")\n",
    "\n",
    "best_path = os.path.join(CFG[\"paths\"][\"out_dir\"], \"swin_best.pth\")\n",
    "\n",
    "print(\"‚úÖ Model ready with dropout, partial freeze, and stronger regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "611cde08-57aa-40fc-9f68-b2dd552a9c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ start\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ start\")\n",
    "all_labels = torch.tensor([y for _, y in train_ds])\n",
    "print(\"Min label:\", all_labels.min().item(), \"Max label:\", all_labels.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd19454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:48<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.402, acc=0.045 | Val: loss=2.226, acc=0.071\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [15:01<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.406, acc=0.039 | Val: loss=2.221, acc=0.093\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:20<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.401, acc=0.048 | Val: loss=2.213, acc=0.142\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:34<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.385, acc=0.059 | Val: loss=2.201, acc=0.243\n",
      "  ‚úÖ Saved new best model\n",
      "‚úÖ Unfroze VideoMAE backbone\n",
      "\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:57<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.332, acc=0.089 | Val: loss=2.179, acc=0.314\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:42<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.290, acc=0.137 | Val: loss=2.156, acc=0.313\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:53<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.221, acc=0.183 | Val: loss=2.133, acc=0.309\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:58<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.177, acc=0.190 | Val: loss=2.119, acc=0.266\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [15:00<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.119, acc=0.184 | Val: loss=2.102, acc=0.259\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:46<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.073, acc=0.198 | Val: loss=2.074, acc=0.477\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:45<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=2.017, acc=0.234 | Val: loss=2.057, acc=0.598\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:38<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.969, acc=0.272 | Val: loss=2.041, acc=0.616\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:44<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.933, acc=0.303 | Val: loss=2.033, acc=0.624\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:38<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.886, acc=0.372 | Val: loss=2.003, acc=0.683\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:44<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.838, acc=0.438 | Val: loss=1.974, acc=0.678\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:48<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.782, acc=0.550 | Val: loss=1.933, acc=0.688\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:26<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.679, acc=0.656 | Val: loss=1.896, acc=0.664\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:33<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.514, acc=0.728 | Val: loss=1.875, acc=0.642\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:35<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.364, acc=0.740 | Val: loss=1.865, acc=0.486\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [15:10<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.194, acc=0.751 | Val: loss=1.812, acc=0.594\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [15:00<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.068, acc=0.779 | Val: loss=1.795, acc=0.537\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:55<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.995, acc=0.765 | Val: loss=1.749, acc=0.645\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:53<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.916, acc=0.810 | Val: loss=1.715, acc=0.701\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [15:15<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.857, acc=0.857 | Val: loss=1.713, acc=0.649\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 331/331 [14:59<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.796, acc=0.889 | Val: loss=1.681, acc=0.695\n",
      "  ‚úÖ Saved new best model\n",
      "\n",
      "üèÅ Best Val Acc = 0.695; Best Val Loss = 1.681\n"
     ]
    }
   ],
   "source": [
    "# 5) Training loop with AMP, early stopping\n",
    "\n",
    "from torch import amp\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "    for X, y in tqdm(loader):\n",
    "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(\"cuda\", dtype=torch.float32):\n",
    "            X = X.permute(0, 2, 1, 3, 4)   # [B,C,T,H,W] -> [B,T,C,H,W]\n",
    "            yhat = model(X).logits\n",
    "            loss = criterion(yhat, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "        correct += (yhat.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        X = X.permute(0, 2, 1, 3, 4)\n",
    "        yhat = model(X).logits\n",
    "        loss_sum += criterion(yhat, y).item() * y.size(0)\n",
    "        correct += (yhat.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "best_val_acc, best_val_loss = 0.0, float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, CFG[\"train\"][\"epochs\"] + 1):\n",
    "    # unfreeze backbone after 5 epochs\n",
    "    if epoch == 5:\n",
    "        for param in model.videomae.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"‚úÖ Unfroze VideoMAE backbone\")\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{CFG['train']['epochs']}\")\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    scheduler.step()\n",
    "    print(f\"Train: loss={tr_loss:.3f}, acc={tr_acc:.3f} | Val: loss={val_loss:.3f}, acc={val_acc:.3f}\")\n",
    "\n",
    "    improved = val_loss < best_val_loss\n",
    "    if improved:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        epochs_no_improve = 0\n",
    "        print(\"  ‚úÖ Saved new best model\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= CFG[\"train\"][\"patience\"]:\n",
    "            print(\"‚èπÔ∏è Early stopping\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nüèÅ Best Val Acc = {best_val_acc:.3f}; Best Val Loss = {best_val_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4600d681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averaged stitched metrics:\n",
      "  raw_acc: 0.711\n",
      "  med_acc: 0.711\n",
      "  vote_acc: 0.711\n",
      "  raw_f1: 0.628\n",
      "  med_f1: 0.628\n",
      "  vote_f1: 0.628\n",
      "\n",
      "Classification report (timeline, voted):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       152\n",
      "           1      0.000     0.000     0.000      1120\n",
      "           4      0.000     0.000     0.000       320\n",
      "           7      0.133     0.635     0.220       416\n",
      "           9      0.855     0.858     0.856      6968\n",
      "\n",
      "    accuracy                          0.695      8976\n",
      "   macro avg      0.198     0.298     0.215      8976\n",
      "weighted avg      0.670     0.695     0.675      8976\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6) Evaluation: per-video stitching with median filter and local majority vote\n",
    "from collections import defaultdict\n",
    "from scipy.signal import medfilt\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_video_clips(ds, vid, clip_len, stride):\n",
    "    # gather validation indices for a given video id\n",
    "    idxs = [i for i,(p, s, y) in enumerate(ds.index) if int(p.name.split('_')[1].split('.')[0]) == int(vid)]\n",
    "    # create sliding windows across the whole video range covered by annots\n",
    "    # For simplicity, reuse ds._read_clip and ds.index starts as anchors\n",
    "    clips, anchors, labels = [], [], []\n",
    "    for i in idxs:\n",
    "        p, s, y = ds.index[i]\n",
    "        x = ds._read_clip(p, s)\n",
    "        x = torch.stack([val_tf(frame) for frame in x])  # per-frame\n",
    "        x = x.permute(1,0,2,3).contiguous()  # [C,T,H,W]\n",
    "        clips.append(x)\n",
    "        anchors.append(s)\n",
    "        labels.append(y)\n",
    "    return clips, anchors, labels\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_stitched(model_path, ds_val, stride=8):\n",
    "    # model = build_swin(num_classes=CFG[\"data\"][\"num_classes\"], pretrained=False).to(device)\n",
    "    # model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    vids = sorted(ds_val.data[\"VideoID\"].unique())\n",
    "    metrics = {}\n",
    "    all_preds, all_tgts = [], []\n",
    "\n",
    "    for vid in vids:\n",
    "        clips, anchors, labels = get_video_clips(ds_val, vid, CFG[\"data\"][\"clip_len\"], stride)\n",
    "        probs = []\n",
    "        for x in clips:\n",
    "            x = x.unsqueeze(0).to(device, non_blocking=True)  # [1,C,T,H,W]\n",
    "            x = x.permute(0, 2, 1, 3, 4)                     # ‚úÖ [1,T,C,H,W]\n",
    "            with torch.inference_mode():\n",
    "                yhat = model(x)\n",
    "                yhat = yhat.logits if hasattr(yhat, \"logits\") else yhat\n",
    "                p = yhat.softmax(1).cpu().numpy()\n",
    "            probs.append(p)\n",
    "            del x\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if len(probs) == 0:\n",
    "            continue\n",
    "        probs = np.concatenate(probs)  # [Nclips, C]\n",
    "        pred_clips = probs.argmax(1)\n",
    "\n",
    "        # Stitching: repeat each clip prediction for 'stride' frames\n",
    "        timeline = np.repeat(pred_clips, stride)\n",
    "        # Median filter to stabilize\n",
    "        k = 7 if len(timeline) >= 7 else (len(timeline)//2*2+1)\n",
    "        timeline_med = medfilt(timeline, kernel_size=k)\n",
    "        # Local voting in a sliding window\n",
    "        window = 11\n",
    "        timeline_vote = np.copy(timeline)\n",
    "        for i in range(len(timeline)):\n",
    "            a = max(0, i - window//2)\n",
    "            b = min(len(timeline), i + window//2 + 1)\n",
    "            timeline_vote[i] = mode(timeline[a:b], keepdims=True)[0][0]\n",
    "\n",
    "        # Build pseudo-ground truth by repeating labels per anchor region length\n",
    "        # (If you have exact frame-level GT, replace this with the true timeline.)\n",
    "        gt = np.repeat(np.array(labels), stride)\n",
    "        n = min(len(gt), len(timeline))\n",
    "        gt, timeline, timeline_med, timeline_vote = gt[:n], timeline[:n], timeline_med[:n], timeline_vote[:n]\n",
    "\n",
    "        m = {\n",
    "            \"raw_acc\":  accuracy_score(gt, timeline),\n",
    "            \"med_acc\":  accuracy_score(gt, timeline_med),\n",
    "            \"vote_acc\": accuracy_score(gt, timeline_vote),\n",
    "            \"raw_f1\":   f1_score(gt, timeline, average=\"macro\"),\n",
    "            \"med_f1\":   f1_score(gt, timeline_med, average=\"macro\"),\n",
    "            \"vote_f1\":  f1_score(gt, timeline_vote, average=\"macro\"),\n",
    "        }\n",
    "        metrics[vid] = m\n",
    "        all_preds.extend(timeline_vote.tolist())\n",
    "        all_tgts.extend(gt.tolist())\n",
    "\n",
    "    # Aggregate metrics\n",
    "    if len(metrics) == 0:\n",
    "        print(\"No videos evaluated. Check paths/annotations.\")\n",
    "        return {}, {}\n",
    "    keys = list(metrics.values())[0].keys()\n",
    "    avg = {k: float(np.mean([metrics[v][k] for v in metrics])) for k in keys}\n",
    "    print(\"\\nAveraged stitched metrics:\")\n",
    "    for k,v in avg.items():\n",
    "        print(f\"  {k}: {v:.3f}\")\n",
    "\n",
    "    # Confusion matrix over all frames (after voting)\n",
    "    cm = confusion_matrix(all_tgts, all_preds, labels=list(range(CFG[\"data\"][\"num_classes\"])))\n",
    "    print(\"\\nClassification report (timeline, voted):\")\n",
    "    print(classification_report(all_tgts, all_preds, digits=3))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    ax.set_title('Confusion Matrix (timeline, voted)')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "    ax.set_xticks(range(CFG[\"data\"][\"num_classes\"]))\n",
    "    ax.set_yticks(range(CFG[\"data\"][\"num_classes\"]))\n",
    "    plt.show()\n",
    "    return avg, metrics\n",
    "\n",
    "avg_metrics, per_vid = evaluate_stitched(best_path, val_ds, stride=CFG[\"data\"][\"stride_eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833ef2d-a440-4bab-9ef2-55f69754d6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
